"""Profile building pipeline."""

import logging
from pathlib import Path

import numpy as np

from zotwatch.config.settings import Settings
from zotwatch.core.exceptions import ProfileBuildError
from zotwatch.core.models import ClusteredProfile, ProfileArtifacts, ZoteroItem
from zotwatch.infrastructure.embedding import (
    CachingEmbeddingProvider,
    EmbeddingCache,
    FaissIndex,
    create_embedding_provider,
)
from zotwatch.infrastructure.embedding.base import BaseEmbeddingProvider
from zotwatch.infrastructure.storage import ProfileStorage
from zotwatch.utils.hashing import hash_content
from zotwatch.utils.temporal import compute_batch_weights

logger = logging.getLogger(__name__)


class ProfileBuilder:
    """Builds user research profile from library."""

    def __init__(
        self,
        base_dir: Path | str,
        storage: ProfileStorage,
        settings: Settings,
        vectorizer: BaseEmbeddingProvider | None = None,
        embedding_cache: EmbeddingCache | None = None,
    ):
        """Initialize profile builder.

        Args:
            base_dir: Base directory for data files.
            storage: Profile storage for items.
            settings: Application settings.
            vectorizer: Optional base embedding provider (defaults to VoyageEmbedding).
            embedding_cache: Optional embedding cache. If provided, wraps vectorizer
                            with CachingEmbeddingProvider for profile source type.
        """
        self.base_dir = Path(base_dir)
        self.storage = storage
        self.settings = settings

        # Create base vectorizer
        base_vectorizer = vectorizer or create_embedding_provider(settings.embedding)

        # Wrap with caching if cache is provided
        if embedding_cache is not None:
            self.vectorizer: BaseEmbeddingProvider = CachingEmbeddingProvider(
                provider=base_vectorizer,
                cache=embedding_cache,
                source_type="profile",
                ttl_days=None,  # Profile embeddings never expire
            )
            self._cache = embedding_cache
        else:
            self.vectorizer = base_vectorizer
            self._cache = None

        self.artifacts = ProfileArtifacts(
            sqlite_path=str(self.base_dir / "data" / "profile.sqlite"),
            faiss_path=str(self.base_dir / "data" / "faiss.index"),
        )

    def _compute_library_hash(self) -> str:
        """Compute a hash representing the current library state.

        Uses item count + all content hashes to detect any change.
        """
        content_hashes = self.storage.get_all_content_hashes()
        # Sort by key for deterministic ordering
        parts = [f"{k}:{v}" for k, v in sorted(content_hashes.items())]
        return hash_content(str(len(parts)), *parts)

    def _can_skip_rebuild(self) -> bool:
        """Check whether the profile can be skipped (nothing changed).

        Returns True if:
        1. FAISS index file exists on disk
        2. Embedding signature matches (same provider/model)
        3. Library hash matches (no items added/removed/changed)
        """
        faiss_path = Path(self.artifacts.faiss_path)
        if not faiss_path.exists():
            logger.info("FAISS index not found, rebuild required")
            return False

        # Check embedding provider/model hasn't changed
        current_signature = self.settings.embedding.signature
        stored_signature = self.storage.get_metadata("embedding_signature")
        if stored_signature != current_signature:
            logger.info(
                "Embedding signature changed (%s -> %s), rebuild required",
                stored_signature,
                current_signature,
            )
            return False

        # Check library content hasn't changed
        current_hash = self._compute_library_hash()
        stored_hash = self.storage.get_metadata("profile_library_hash")
        if stored_hash != current_hash:
            logger.info("Library content changed, rebuild required")
            return False

        logger.info("Profile is up to date, skipping rebuild")
        return True

    def run(self, *, full: bool = False) -> ProfileArtifacts:
        """Build profile from library items.

        Args:
            full: If True, invalidate all profile embeddings and recompute.
                  If False (default), use cached embeddings where available,
                  and skip rebuild entirely if nothing changed.
        """
        # Skip rebuild if nothing changed (incremental mode only)
        if not full and self._can_skip_rebuild():
            return self.artifacts

        total_items = self.storage.count_items()
        items = self.storage.get_items_with_abstract()

        if total_items == 0:
            raise ProfileBuildError("No items found in storage; run ingest before building profile.")

        items_without_abstract = total_items - len(items)

        logger.info(
            "Library statistics: %d items with abstract, %d items without abstract",
            len(items),
            items_without_abstract,
        )

        if not items:
            raise ProfileBuildError(
                "No items with abstracts found in storage; profile building requires items with abstracts."
            )

        logger.info("Building profile from %d items (with abstracts)", len(items))

        # If full rebuild requested and cache is available, invalidate profile embeddings
        if full and self._cache is not None:
            invalidated = self._cache.invalidate_source("profile")
            if invalidated > 0:
                logger.info("Invalidated %d cached profile embeddings for full rebuild", invalidated)

        # Encode all items (caching handled automatically by CachingEmbeddingProvider)
        texts = [item.content_for_embedding() for item in items]
        source_ids = [item.key for item in items]

        # Use encode_with_ids if available (for source tracking)
        if isinstance(self.vectorizer, CachingEmbeddingProvider):
            vectors = self.vectorizer.encode_with_ids(texts, source_ids=source_ids)
        else:
            vectors = self.vectorizer.encode(texts)

        logger.info("Computed embeddings for %d items", len(items))

        # Build FAISS index
        logger.info("Building FAISS index")
        index, _ = FaissIndex.from_vectors(vectors)
        index.save(self.artifacts.faiss_path)

        # Persist embedding signature to detect provider/model changes across runs
        signature = self.settings.embedding.signature
        self.storage.set_metadata("embedding_signature", signature)

        # Persist library hash to detect content changes across runs
        library_hash = self._compute_library_hash()
        self.storage.set_metadata("profile_library_hash", library_hash)

        # Run clustering if enabled
        if self.settings.profile.clustering.enabled:
            self._run_clustering(vectors, items, signature)

        return self.artifacts

    def _run_clustering(
        self,
        vectors: np.ndarray,
        items: list[ZoteroItem],
        embedding_signature: str,
    ) -> ClusteredProfile | None:
        """Run k-means clustering on profile embeddings.

        Args:
            vectors: Embedding matrix (N x dim).
            items: Corresponding ZoteroItem list.
            embedding_signature: Embedding provider and model signature.

        Returns:
            ClusteredProfile if clustering was successful, None otherwise.
        """
        from zotwatch.pipeline.profile_clusterer import ProfileClusterer

        clusterer = ProfileClusterer(
            config=self.settings.profile.clustering,
            embedding_signature=embedding_signature,
        )

        # Compute temporal weights if enabled
        temporal_weights = None
        temporal_config = self.settings.profile.clustering.temporal
        if temporal_config.enabled:
            weights_list = compute_batch_weights(
                items,
                halflife_days=temporal_config.halflife_days,
                min_weight=temporal_config.min_weight,
            )
            temporal_weights = np.array(weights_list, dtype=np.float32)
            logger.info(
                "Computed temporal weights for %d items (halflife=%.0f days, mean_weight=%.3f)",
                len(items),
                temporal_config.halflife_days,
                np.mean(temporal_weights),
            )

        clustered_profile = clusterer.cluster(vectors.copy(), items, temporal_weights=temporal_weights)

        if clustered_profile.valid_cluster_count == 0:
            logger.info("No valid clusters created (library may be too small)")
            # Clear stale clustered profile cache to prevent Ranker from loading outdated data
            self.storage.clear_clustered_profile_cache()
            return None

        # Save to storage
        self.storage.save_clustered_profile(clustered_profile)

        logger.info(
            "Created %d valid clusters covering %d/%d papers (total_effective_size=%.2f)",
            clustered_profile.valid_cluster_count,
            clustered_profile.papers_in_valid_clusters,
            clustered_profile.total_papers,
            clustered_profile.total_effective_size,
        )

        return clustered_profile


__all__ = ["ProfileBuilder"]
